# -*- coding: utf-8 -*-
"""dl_a2_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RLYEAVVWHXsHslzyYfd6BZBo6oj1eBQ6
"""

from google.colab import drive
drive.mount('/content/drive')

!unzip /content/drive/MyDrive/Assignments/A2/train.zip
!unzip /content/drive/MyDrive/Assignments/A2/test.zip

import os
import torch
import torchvision
import pandas as pd
import imageio as io
from torch import nn
from torch import optim
from torchvision import utils
from torchsummary import summary
import matplotlib.pyplot as plt
import torch.nn.functional as F
from torchvision import transforms
from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau
from torch.utils.data import Dataset, random_split, DataLoader
from sklearn.metrics import confusion_matrix, classification_report

if torch.cuda.is_available():
    device = "cuda:0"
else:
    device = "cpu"
device

#paths
TRAIN_CSV = "/content/train.csv"
TRAIN_DATA ="/content/train_new"
TEST_CSV = "/content/test.csv"
TEST_DATA = "/content/test_new"

#loader
TRAIN_BS = 1000
VAL_BS = 1000
TEST_BS = 1000

#NN Models
NO_OF_LAYERS = 3
INPUT_DIM = 28*28
# NEURONS_PER_LAYER = [392, 10]
# NEURONS_PER_LAYER = [192, 10]
NEURONS_PER_LAYER = [128, 64, 10]
# NEURONS_PER_LAYER = [392, 98, 10]
# NEURONS_PER_LAYER = [392, 128, 64, 10]
# NEURONS_PER_LAYER = [392, 196, 49, 10]
NN_DROPOUT = 0

#CNN Models
#A1
#[(in_channels, outchannels, kernel_size, strike, dropout=0)]
A1_CONVS = [(1, 16, 5, 2), (16, 8, 3, 1)]
A1_FCS = [800, 10]
#A2
A2_CONVS = [(1, 32, 5, 2), (32, 16, 3, 1), (16, 8, 3, 1)]
A2_FCS = [512, 256, 10]
#A3
A3_CONVS = [(1, 32, 5, 2), (32, 16, 3, 1), (16, 8, 3, 1),(8, 4, 3, 1)]
A3_FCS = [144, 72, 10]
#A4
A4_CONVS = [(1, 32, 5, 2), (32, 16, 3, 1), (16, 8, 3, 1),(8, 4, 3, 1),(4, 2, 3, 1)]
A4_FCS = [32, 10]

#select the model that you want to test. (These models are shown in the report)
CONVS = A1_CONVS
FCS = A1_FCS
CNN_DROPOUT = 0


#training parameters
LOSS_FN = nn.CrossEntropyLoss()
LR = 0.01
TRAINING_EPOCHS =5

class MNIST(Dataset):
    def __init__(self, csv_file_dir, data_dir, transform=None):
        file = pd.read_csv(csv_file_dir, skiprows=1)
        self.X = file.iloc[:, 0]
        self.y = file.iloc[:, 1]
        self.shape = len(self.X)
        self.data_dir = data_dir
        self.transform = transform

    
    def __getitem__(self, index):
        path = os.path.join(self.data_dir, self.X[index])
        image = io.imread(path)
        label = self.y[index]
        if self.transform:
            image = self.transform(image)
        
        return (image, label)

    def __len__(self):
        return self.shape

def load_data(train_csv, train_content, test_csv, test_content, val_ratio=0.2):
    train_dataset = MNIST(csv_file_dir=train_csv, data_dir=train_content,
        transform=transforms.ToTensor())
    test_dataset = MNIST(csv_file_dir=test_csv, data_dir=test_content,
        transform=transforms.ToTensor())

    size = len(train_dataset)
    val_size = int(size*val_ratio)
    train_size = size-val_size
    train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])
    print(len(train_dataset), len(val_dataset), len(test_dataset))
    return train_dataset, val_dataset, test_dataset

def dataset_check(dataset, index):
    img = dataset[index][0].transpose(0, 1).transpose(1, 2)
    plt.imshow(torch.squeeze(img))
    plt.show()
    print(f"class is {dataset[index][1]}")

def get_loaders(train_dataset, train_BS, val_dataset, val_BS, test_dataset, test_BS):
    train_loader = DataLoader(train_dataset, batch_size=train_BS, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=val_BS, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=test_BS, shuffle=True)
    return train_loader, val_loader, test_loader

def get_distribution(dataloader):
    dis = torch.zeros(10).to(device)
    for batch in dataloader:
        dis+=torch.bincount(batch[1].to(device)).to(device)
    return dis

def plot(distribution, title):
    plt.bar(torch.arange(0,10), distribution)
    plt.xlabel("numbers")
    plt.ylabel("frequency")
    plt.title(title)
    plt.show()

class Block(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, padding, dropout=0):
        super(Block, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, padding)
        self.bn = nn.BatchNorm2d(out_channels)
        self.dropout = nn.Dropout(dropout, inplace=True)
    
    def forward(self, x):
        return F.relu(self.bn(self.conv(x)))

class CNN(nn.Module):
    def __init__(self, conv_layers, fc_layers, dropout=0):
        super(CNN, self).__init__()
        self.input_channels = conv_layers[0][0]
        self.no_of_cnn_layers = len(conv_layers)
        self.no_of_fc_layers = len(fc_layers)
        self.blocks = nn.Sequential()
        self.fcs = nn.Sequential()
        for i, parameters in enumerate(conv_layers):
            self.blocks.add_module("b"+str(i), Block(*parameters))
        for i in range(len(fc_layers)-1):
            self.fcs.add_module("fc"+str(i), nn.Linear(fc_layers[i], fc_layers[i+1]))
        self.dropout = nn.Dropout(dropout, inplace= True)
        
    def forward(self, x):
        out = x.clone()
        for a in self.blocks:
            out = a(out)
        out = out.view(out.size(0), -1)
        for i, f in enumerate(self.fcs):
            if i!=self.no_of_fc_layers-1:
                out = F.sigmoid(f(out))
            else:
                out = self.dropout(out)
                out = f(out)
        return out

class FC(nn.Module):
    def __init__(self,no_of_layers, input_dim, neurons_per_layer, dropout=0):
        super(FC, self).__init__()
        self.layers = nn.Sequential()
        for i in range(no_of_layers):
            if i == 0:
                self.layers.add_module("fc"+str(i), nn.Linear(input_dim, neurons_per_layer[i]))
            else:
                self.layers.add_module("fc"+str(i), nn.Linear(neurons_per_layer[i-1],neurons_per_layer[i]))

    def forward(self, x):
        out = x.clone().view(x.size(0), -1)
        for layer in self.layers:
            out = F.sigmoid(layer(out))
        return out

def init_network_nn(no_of_layers, input_dim, neurons_per_layer, dropout=0):
    return FC(no_of_layers, input_dim, neurons_per_layer, dropout)

def init_network_cnn(convs_layers, fc_layers, dropout=0):
    return CNN(convs_layers, fc_layers, dropout)

def plot_graph(results, legend, legend_loc, xlabel, ylabel):
    for r in results:
        plt.plot(r, linewidth=1)
    plt.legend(legend, loc=legend_loc)
    plt.xlabel(xlabel, fontsize=15)
    plt.ylabel(ylabel, fontsize=15)
    plt.show()

def train(net, train_loader, val_loader, training_epochs, loss_func, optimizer, scheduler=None):
    print("training models...")
    val_loss = []
    val_accu = []
    train_loss = []
    train_accu = []
    N = 0;
    for epoch in range(training_epochs):
        net.train()
        t_loss = 0
        correct = 0
        count = 0
        for i, (images, labels) in enumerate(train_loader):
            images = images.to(device)
            labels = labels.to(device)
            output = net(images)
            loss = loss_func(output, labels)
            optimizer.zero_grad() #WHY??
            loss.backward()
            optimizer.step()
            t_loss += loss.item() # sum of all batches loss
            correct += ((output.detach().argmax(dim=1) == labels).sum()/images.shape[0]) #sum of accuracies
            count = i
        count+=1
        train_loss.append((t_loss/count)*100) # sum/total number of batches
        train_accu.append((correct/count)*100) # sum/total number of batches
        print(f"epoch no {epoch}")
        print(f"training loss: {train_loss[-1]:.3f}, training accuracy: {train_accu[-1]:.3f}")

        net.eval()
        t_loss = 0
        correct = 0
        count = 0
        for i, (images, labels) in enumerate(val_loader):
            images = images.to(device)
            labels = labels.to(device)
            output = net(images)
            loss = loss_func(output, labels)
            t_loss += loss.item()
            correct += ((output.detach().argmax(dim=1) == labels).sum()/images.shape[0])
            count =i
        count+=1
        val_loss.append((t_loss/count)*100)
        val_accu.append((correct/count)*100)
        print(f"val loss: {val_loss[-1]:.3f}, val accuracy: {val_accu[-1]:.3f}")
        if(val_accu[-1]<train_accu[-1] and train_accu[-1]>90):
            N+=1
        else:
            N = 0
        if N == 5:
            break
        if scheduler != None:
            scheduler.step()
    plot_graph([train_loss,val_loss] ,["training loss", "validation loss"],"upper left", "epoch", "loss")
    plot_graph([train_accu, val_accu], ["training accu", "validation accu"], "upper left", "epoch", "accuracy")
    return net

def save_model(net, path):
    print("saving model...")
    torch.save(net.state_dict(), path)

def load_model(net, path):
    print("loading model...")
    state_dict = torch.load(path, map_location='cpu')
    net.load_state_dict(state_dict)

def test(net, loader):
    print("testing model...")
    x = None
    y = None
    y_pred = None
    for images, labels in loader:
        images = images.to(device)
        output = F.softmax(net(images))
        output = torch.argmax(output, dim=1)
        if y_pred == None:
            y_pred = output
            y = labels
            x = images
        else:
            y_pred = torch.cat((y_pred, output), dim=0)
            y = torch.cat((y, labels), dim=0)
            x = torch.cat((x, images), dim=0)
    y_pred = y_pred.to("cpu")
    acc = ((y_pred == y).sum()/y.shape[0])*100
    print(f"test dataset accuracy is {acc}")
    return x, y, y_pred

import seaborn as sns
def plot_confu(model, loader):
    print("plotting confusion matrix...")
    y = None
    y_pred = None
    for images, labels in loader:
        images = images.to(device)
        output = F.softmax(model(images))
        output = torch.argmax(output, dim=1)
        if y_pred == None:
            y_pred = output
            y = labels
        else:
            y_pred = torch.cat((y_pred, output), dim=0)
            y = torch.cat((y, labels), dim=0)
    
    y_pred = y_pred.to("cpu")
    fig, ax = plt.subplots(figsize=(15, 5))
    cm = confusion_matrix(y, y_pred, normalize="true")
    ax = sns.heatmap(cm, annot=True, cmap='Blues')
    plt.show()

def show_true_pred(images, labels, y_pred):
    print("showing true preditionss")
    done = torch.zeros(10)
    count = 0
    i =0
    fig, axes = plt.subplots(1, 4,figsize=(12,4))
    fig.suptitle('True Predictions')
    while count<4:
        if y_pred[i] == labels[i] and done[labels[i]] == 0:
            done[labels[i]] = 1
            img = images[i]
            axes[count].imshow(torch.squeeze(img))
            axes[count].set_title(f"label: {labels[i]}")
            count+=1
        i+=1
    plt.show()
    done = torch.zeros(10)

def show_false_pred(images, labels, y_pred):
    print("showing wrong predictions...")
    done = torch.zeros(10)
    count = 0
    i =0
    fig, axes = plt.subplots(1, 4,figsize=(12,4))
    fig.suptitle('Wrong Predictions')
    while count<4:
        if y_pred[i] != labels[i] and done[labels[i]] == 0:
            done[labels[i]] = 1
            img = images[i]
            axes[count].imshow(torch.squeeze(img))
            axes[count].set_title(f"Predicted: {y_pred[i]}")
            count+=1
        i+=1
    plt.show()

import numpy as np
def visTensor(tensor, ch=0, allkernels=False, nrow=8, padding=1): 
    print("plotting learned filters")
    '''https://stackoverflow.com/questions/55594969/how-to-visualise-filters-in-a-cnn-with-pytorch'''
    '''This function is taken from this link'''
    '''It is used to display the filter learned by any layer'''
    n,c,w,h = tensor.shape

    if allkernels: tensor = tensor.view(n*c, -1, w, h)
    elif c != 3: tensor = tensor[:,ch,:,:].unsqueeze(dim=1)

    rows = np.min((tensor.shape[0] // nrow + 1, 64))    
    grid = utils.make_grid(tensor, nrow=nrow, normalize=True, padding=padding)
    plt.figure( figsize=(nrow,rows) )
    plt.imshow(grid.numpy().transpose((1, 2, 0)))
    plt.axis('off')
    plt.ioff()
    plt.show()

from sklearn.manifold import TSNE
def plot_tsne(labels, y_pred):
    print("plotting tsne...")
    tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
    tsne_result = tsne.fit_transform(y_pred)
    fig, ax = plt.subplots()
    fig.set_size_inches(10, 8)
    hue_labels = labels
    sns.scatterplot(tsne_result[:,0], tsne_result[:,1], hue=hue_labels, legend='full')
    plt.show()

from sklearn.metrics import roc_curve, auc
def plot_rocs(labels, y_pred):
    print("plotting rocs...")
    fpr = dict()
    tpr = dict()
    roc_auc = dict()

    for i in range(10):
        fpr[i], tpr[i], _ = roc_curve(y_pred, labels, pos_label=i)
        roc_auc[i] = auc(fpr[i], tpr[i])

    for i in range(10):
        plt.figure()
        plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC of {i} Class')
        plt.legend(loc="lower right")
        plt.show()

def main(network_type, is_training=True, save_model=True, plot_distributions=True, print_summary=True,
         confusion_matrix=True, PRF1=True, true_false_predictions=True, 
         show_last_layer_parameters=True, ROCs_plots=True):
    print("In main...")
    print(f"device available is {device}")
    train_dataset, val_dataset, test_dataset = load_data(TRAIN_CSV, TRAIN_DATA, TEST_CSV, TEST_DATA)
    train_loader, val_loader, test_loader = get_loaders(train_dataset, TRAIN_BS, val_dataset, VAL_BS, test_dataset, TEST_BS )
    if plot_distributions:
        train_distribution = get_distribution(train_loader).to("cpu")
        val_distribution = get_distribution(val_loader).to("cpu")
        test_distribution = get_distribution(test_loader).to("cpu")
        plot(train_distribution, "Training Data")
        plot(val_distribution, "Validation Data")
        plot(test_distribution, "Testing Data")
    torch.cuda.empty_cache()
    if network_type == "nn":
        net = init_network_nn(NO_OF_LAYERS, INPUT_DIM, NEURONS_PER_LAYER);
        print(net)
        net = net.to(device)
    if network_type == "cnn":
        net = init_network_cnn(CONVS, FCS)
        print(net)
        net = net.to(device)
    if print_summary:
        summary(net, (1, 28, 28))
    if is_training == True:
        optimizer = optim.Adam(net.parameters(), lr = LR)
        scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
        net = train(net, train_loader, val_loader, TRAINING_EPOCHS, LOSS_FN, optimizer)
    else:
        if network_type == "nn":
            load_model(net, "/content/drive/MyDrive/Assignments/A2/submission/t1.pt")
        elif network_type == "cnn":
            load_model(net, "/content/drive/MyDrive/Assignments/A2/submission/t2.pt")
    if save_model:
        save_model(net, "/content/drive/MyDrive/Assignments/A2/submission/model.pt")
    net = net.to(device="cpu")
    x, y, y_pred = test(net, test_loader)
    print(f"y_pred.shape : {y_pred.shape}")
    if confusion_matrix:
        plot_confu(net, test_loader)
    if PRF1:
        print(classification_report(y, y_pred, digits=3))
    if true_false_predictions:
        show_true_pred(x, y, y_pred)
        show_false_pred(x, y, y_pred)
    if show_last_layer_parameters and network_type == "cnn":
        filter = net.blocks[1].conv.weight.data.clone()
        visTensor(filter, ch=0, allkernels=False)
    if ROCs_plots:
        plot_rocs(y, y_pred)

main(network_type = "cnn",
     is_training=False,
     save_model = False,
     plot_distributions=False, 
     print_summary=True,
     confusion_matrix=True, 
     PRF1=True, 
     true_false_predictions=True, 
     show_last_layer_parameters=True, 
     ROCs_plots=True
     )